{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4cc30c7",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc6bdd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class TransformerSkip(nn.Module):\n",
    "    def __init__(self, channels, nhead=4, dim_feedforward=512):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=channels,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        tokens = x.view(B, C, -1).permute(0, 2, 1)      \n",
    "        tokens = self.transformer(tokens)              \n",
    "        x_out = tokens.permute(0, 2, 1).view(B, C, H, W)\n",
    "        return x_out\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.pool    = nn.MaxPool2d(2)\n",
    "        self.conv    = DoubleConv(in_ch, out_ch)\n",
    "        self.skip_tr = TransformerSkip(out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        skip = self.skip_tr(x)\n",
    "        return x, skip\n",
    "\n",
    "class Up(nn.Module):\n",
    "\n",
    "    def __init__(self, dec_ch, skip_ch, out_ch, bilinear=True):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(dec_ch, dec_ch, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(dec_ch + skip_ch, out_ch)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "\n",
    "        x = torch.cat([skip, x], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNetWithTransformerSkips(nn.Module):\n",
    "    def __init__(self, in_channels=38, n_classes=3, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.inc    = DoubleConv(in_channels, 32)\n",
    "        self.skip0  = TransformerSkip(32)\n",
    "\n",
    "        self.down1  = Down(32,  64)\n",
    "        self.down2  = Down(64, 128)\n",
    "        self.down3  = Down(128, 256)\n",
    "\n",
    "        self.bottleneck = DoubleConv(256, 256)\n",
    "\n",
    "        self.up1 = Up(dec_ch=256, skip_ch=128, out_ch=128, bilinear=bilinear)\n",
    "        self.up2 = Up(dec_ch=128, skip_ch=64, out_ch=64, bilinear=bilinear)\n",
    "        self.up3 = Up(dec_ch=64, skip_ch=32,  out_ch=32,  bilinear=bilinear)\n",
    "\n",
    "        self.seg_head = nn.Conv2d(32, n_classes, kernel_size=1)\n",
    "        self.reg_head = nn.Conv2d(32, 1,         kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.inc(x)         \n",
    "        s0 = self.skip0(x0)      \n",
    "\n",
    "     \n",
    "        x1, s1 = self.down1(x0)\n",
    "        x2, s2 = self.down2(x1)  \n",
    "        x3, _  = self.down3(x2)  \n",
    "\n",
    "  \n",
    "        b = self.bottleneck(x3) \n",
    "\n",
    " \n",
    "        u1 = self.up1(b,  s2)     \n",
    "        u2 = self.up2(u1, s1)     \n",
    "        u3 = self.up3(u2, s0)    \n",
    "\n",
    "        seg_logits = self.seg_head(u3)  \n",
    "        reg_map    = self.reg_head(u3)  \n",
    "        with torch.no_grad():\n",
    "            predicted_class = torch.argmax(seg_logits, dim=1, keepdim=True) \n",
    "            class1_mask = (predicted_class == 1).float()                    \n",
    "            masked_reg = reg_map * class1_mask \n",
    "\n",
    "        return seg_logits, masked_reg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b83db3",
   "metadata": {},
   "source": [
    "**FirstDataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df06b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "arr = np.load(\"ReadytoTrainDLNew2.npy\")\n",
    "features = arr[:, :38, :, :].astype(np.float32)    \n",
    "rates    = arr[:, 40, :, :].astype(np.float32)     \n",
    "flags    = arr[:, 39, :, :].astype(np.int64)\n",
    "\n",
    "del arr\n",
    "\n",
    "eps       = 1e-4\n",
    "log_rates = np.log(rates + eps)                  \n",
    "\n",
    "mean = features.mean(axis=(0,2,3), keepdims=True)\n",
    "std  = features.std(axis=(0,2,3), keepdims=True)\n",
    "features = (features - mean) / (std + 1e-6)\n",
    "\n",
    "np.save('std.npy',std)\n",
    "np.save('mean.npy',mean)\n",
    "\n",
    "X_train, X_test, y_log_train, y_log_test, y_flag_train, y_flag_test = train_test_split(\n",
    "    features, log_rates, flags,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "del log_rates,features,flags\n",
    "\n",
    "class PrecipDataset(Dataset):\n",
    "    def __init__(self, feats, log_r, flags):\n",
    "        self.X      = torch.from_numpy(feats)               \n",
    "        self.y_rate = torch.from_numpy(log_r)               \n",
    "        self.y_flag = torch.from_numpy(flags)              \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y_flag[idx], self.y_rate[idx]\n",
    "\n",
    "train_ds = PrecipDataset(X_train, y_log_train, y_flag_train)\n",
    "test_ds  = PrecipDataset(X_test,  y_log_test,  y_flag_test)\n",
    "\n",
    "del X_train, y_log_train, y_flag_train, X_test, y_log_test, y_flag_test\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=4)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "del train_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5051ff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = UNetWithTransformerSkips(in_channels=38, n_classes=2).to(device)\n",
    "\n",
    "\n",
    "freq = torch.tensor([1, 1], device=device)\n",
    "class_weights = (1.0 / freq)\n",
    "class_weights = class_weights * (2.0 / class_weights.sum())\n",
    "\n",
    "seg_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "reg_criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2 / 2)\n",
    "\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=15, T_mult=1, eta_min=1e-6)\n",
    "\n",
    "lambda_reg = 0.09\n",
    "num_epochs = 30\n",
    "\n",
    "TRAIN = []\n",
    "VAL = []\n",
    "VAL_SEG = []\n",
    "VAL_REG = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (X, flags, log_rates) in enumerate(train_loader):\n",
    "        X, flags, log_rates = X.to(device), flags.to(device), log_rates.to(device)\n",
    "        seg_logits, reg_map = model(X)\n",
    "\n",
    "        loss_seg = seg_criterion(seg_logits, flags)\n",
    "        loss_reg = reg_criterion(reg_map.squeeze(1), log_rates)\n",
    "        loss = loss_seg + lambda_reg * loss_reg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(epoch + batch_idx / len(train_loader))\n",
    "\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_seg_loss = 0.0\n",
    "    val_reg_loss = 0.0\n",
    "    correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, flags, log_rates in test_loader:\n",
    "            X, flags, log_rates = X.to(device), flags.to(device), log_rates.to(device)\n",
    "\n",
    "            seg_logits, reg_map = model(X)\n",
    "            loss_seg = seg_criterion(seg_logits, flags)\n",
    "            loss_reg = reg_criterion(reg_map.squeeze(1), log_rates)\n",
    "            loss = loss_seg + lambda_reg * loss_reg\n",
    "\n",
    "            batch_size = X.size(0)\n",
    "            val_loss += loss.item() * batch_size\n",
    "            val_seg_loss += loss_seg.item() * batch_size\n",
    "            val_reg_loss += loss_reg.item() * batch_size\n",
    "\n",
    "            preds = seg_logits.argmax(dim=1)\n",
    "            correct_pixels += (preds == flags).sum().item()\n",
    "            total_pixels += flags.numel()\n",
    "\n",
    "    val_loss /= len(test_loader.dataset)\n",
    "    val_seg_loss /= len(test_loader.dataset)\n",
    "    val_reg_loss /= len(test_loader.dataset)\n",
    "    val_acc = correct_pixels / total_pixels\n",
    "    TRAIN.append(train_loss)\n",
    "    VAL.append(val_loss)\n",
    "    VAL_SEG.append(val_seg_loss)\n",
    "    VAL_REG.append(val_reg_loss)\n",
    "    \n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\" Val Loss: {val_loss:.4f} (Seg: {val_seg_loss:.4f}, Reg: {val_reg_loss:.4f}) | \"\n",
    "          f\" Seg Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_seg_loss < 0.15:\n",
    "        torch.save(model.state_dict(), \"model_weights_R2.pth\")\n",
    "        np.save('TRAIN4.npy', np.array(TRAIN))\n",
    "        np.save('VAL4.npy', np.array(VAL))\n",
    "        np.save('VAL_SEG4.npy', np.array(VAL_SEG))\n",
    "        np.save('VAL_REG4.npy', np.array(VAL_REG))\n",
    "        \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a16068",
   "metadata": {},
   "source": [
    "**SecondDataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a1532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "arr = np.load(\"FinalDATANew2.npy\")\n",
    "\n",
    "\n",
    "features = arr[:, :38, :, :].astype(np.float32)    \n",
    "rates    = arr[:, 40, :, :].astype(np.float32)    \n",
    "flags    = arr[:, 39, :, :].astype(np.int64)       \n",
    "\n",
    "\n",
    "eps       = 1e-4\n",
    "log_rates = np.log(rates + eps)                \n",
    "\n",
    "mean = np.load('mean.npy')\n",
    "std  = np.load('std.npy')\n",
    "features = (features - mean) / (std + 1e-6)\n",
    "\n",
    "X_train, X_test, y_log_train, y_log_test, y_flag_train, y_flag_test = train_test_split(\n",
    "    features, log_rates, flags,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "class PrecipDataset(Dataset):\n",
    "    def __init__(self, feats, log_r, flags):\n",
    "        self.X      = torch.from_numpy(feats)              \n",
    "        self.y_rate = torch.from_numpy(log_r)               \n",
    "        self.y_flag = torch.from_numpy(flags)              \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y_flag[idx], self.y_rate[idx]\n",
    "\n",
    "train_ds = PrecipDataset(X_train, y_log_train, y_flag_train)\n",
    "test_ds  = PrecipDataset(X_test,  y_log_test,  y_flag_test)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=4)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b898dc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNetWithTransformerSkips(in_channels=38, n_classes=2).to(device)\n",
    "model.load_state_dict(torch.load('model_weights_R22.pth'))\n",
    "\n",
    "for p in model.inc.double_conv.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "for down in (model.down1, model.down2, model.down3):\n",
    "    for p in down.conv.double_conv.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "\n",
    "for name, p in model.named_parameters():\n",
    "    if \"double_conv\" in name and any(x in name for x in [\"inc\", \"down1\", \"down2\", \"down3\"]):\n",
    "        assert not p.requires_grad\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623ec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "freq = torch.tensor([1, 1], device=device)\n",
    "class_weights = (1.0 / freq)\n",
    "class_weights = class_weights * (2.0 / class_weights.sum())\n",
    "\n",
    "seg_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "reg_criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "lambda_reg = 0.2\n",
    "num_epochs = 30\n",
    "\n",
    "TRAIN = []\n",
    "VAL = []\n",
    "VAL_SEG = []\n",
    "VAL_REG = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    " \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (X, flags, log_rates) in enumerate(train_loader):\n",
    "        X, flags, log_rates = X.to(device), flags.to(device), log_rates.to(device)\n",
    "        seg_logits, reg_map = model(X)\n",
    "\n",
    "        loss_seg = seg_criterion(seg_logits, flags)\n",
    "        loss_reg = reg_criterion(reg_map.squeeze(1), log_rates)\n",
    "        loss = loss_seg + lambda_reg * loss_reg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_seg_loss = 0.0\n",
    "    val_reg_loss = 0.0\n",
    "    correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, flags, log_rates in test_loader:\n",
    "            X, flags, log_rates = X.to(device), flags.to(device), log_rates.to(device)\n",
    "\n",
    "            seg_logits, reg_map = model(X)\n",
    "            loss_seg = seg_criterion(seg_logits, flags)\n",
    "            loss_reg = reg_criterion(reg_map.squeeze(1), log_rates)\n",
    "            loss = loss_seg + lambda_reg * loss_reg\n",
    "\n",
    "            batch_size = X.size(0)\n",
    "            val_loss += loss.item() * batch_size\n",
    "            val_seg_loss += loss_seg.item() * batch_size\n",
    "            val_reg_loss += loss_reg.item() * batch_size\n",
    "\n",
    "            preds = seg_logits.argmax(dim=1)\n",
    "            correct_pixels += (preds == flags).sum().item()\n",
    "            total_pixels += flags.numel()\n",
    "\n",
    "    val_loss /= len(test_loader.dataset)\n",
    "    val_seg_loss /= len(test_loader.dataset)\n",
    "    val_reg_loss /= len(test_loader.dataset)\n",
    "    val_acc = correct_pixels / total_pixels\n",
    "    TRAIN.append(train_loss)\n",
    "    VAL.append(val_loss)\n",
    "    VAL_SEG.append(val_seg_loss)\n",
    "    VAL_REG.append(val_reg_loss)\n",
    "\n",
    "    \n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\" Val Loss: {val_loss:.4f} (Seg: {val_seg_loss:.4f}, Reg: {val_reg_loss:.4f}) | \"\n",
    "          f\" Seg Acc: {val_acc:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"model_weights_JustRAINTT.pth\")\n",
    "np.save('TRAIN4T.npy', np.array(TRAIN))\n",
    "np.save('VAL4T.npy', np.array(VAL))\n",
    "np.save('VAL_SEG4T.npy', np.array(VAL_SEG))\n",
    "np.save('VAL_REG4T.npy', np.array(VAL_REG))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e585b",
   "metadata": {},
   "source": [
    "# Retrieving Rainfall on a Real Orbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4e20be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "def CollectingData(filename):\n",
    "    \n",
    "    \n",
    "    era5_features = ['tciw', 'tclw', 'tcwv', 't2m', 'cape', 'u10', 'v10', 'skt', 'asn','rsn', 'tcslw', 'tcw', 'lsm', 'mtpr', 'msr']\n",
    "\n",
    "    \n",
    "    dataset = h5py.File(filename, 'r')\n",
    "    \n",
    "    final = []\n",
    "    for variables in era5_features: \n",
    "        data = dataset[f'/ERA5/{variables}']\n",
    "        final.append(data)\n",
    "    \n",
    "    ERA5_Data = np.stack(final, axis=2)\n",
    "    \n",
    "    Latitude = dataset['/NewGrids/Latitude'][:]\n",
    "    Longitude = dataset['/NewGrids/Longitude'][:]\n",
    "\n",
    "\n",
    "    SP_Data = np.stack([Latitude,Longitude], axis=2)\n",
    "    \n",
    "    \n",
    "    Tc = dataset['/NewGrids/Tc'][:]\n",
    "    incidenceAngle = np.expand_dims(dataset['/NewGrids/incidenceAngle'][:][:,:,0], axis = 2)\n",
    "\n",
    "    ATMS_data = np.concatenate((Tc,incidenceAngle), axis=2)\n",
    "\n",
    "\n",
    "    Final_Data = np.concatenate((ATMS_data,SP_Data,ERA5_Data), axis=2)\n",
    "    \n",
    "    Final_Data = np.transpose(Final_Data, (2, 0, 1))\n",
    "    \n",
    "    Rain = (Final_Data[-2]-Final_Data[-1])*3600\n",
    "    Snow = (Final_Data[-1])*3600\n",
    "    R = ((Rain>0.05)&(Snow<0.025))\n",
    "    S = (Snow>0.025)\n",
    "    flag = np.zeros_like(Final_Data[-1])\n",
    "    rate = np.zeros_like(Final_Data[-1])\n",
    "    flag[S]=2\n",
    "    flag[R]=1\n",
    "    rate[S]=Snow[S]\n",
    "    rate[R]=Rain[R]\n",
    "    \n",
    "\n",
    "    \n",
    "    return np.concatenate((Final_Data[:-2],np.expand_dims(rate, axis=0),np.expand_dims(flag, axis=0)), axis=0)\n",
    "\n",
    "def Expand(Data): \n",
    "\n",
    "    if Data.ndim < 4: \n",
    "        Data = np.expand_dims(Data, axis=0)\n",
    "\n",
    "\n",
    "    Rate = np.zeros((Data.shape[0],2,Data.shape[-2],Data.shape[-1]))\n",
    "\n",
    "    for i in range(Data.shape[0]): \n",
    "\n",
    "        RainMask = Data[i,-1]==1\n",
    "        SnowMask = Data[i,-1]==2\n",
    "        Rate[i,0][RainMask] = Data[i,-2][RainMask]\n",
    "        Rate[i,1][SnowMask] = Data[i,-2][SnowMask]\n",
    "\n",
    "\n",
    "    return np.concatenate((Data, Rate ), axis = 1).squeeze()\n",
    "\n",
    "\n",
    "def RerievalDL(model,X,mean,std): \n",
    "\n",
    "        \n",
    "    if X.ndim < 4: \n",
    "        X = np.expand_dims(X, axis=0)\n",
    "\n",
    "    X = (X - mean) / (std + 1e-6)    \n",
    "\n",
    "    eps = 1e-4  # must match what you used in training\n",
    "    model.eval()\n",
    "    Flag = np.zeros((X.shape[0],16,16))\n",
    "    Rain_rate = np.zeros((X.shape[0],16,16))\n",
    "    Snow_rate = np.zeros((X.shape[0],16,16))\n",
    "\n",
    "    for i in range(X.shape[0]): \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            \n",
    "            seg_logits, reg_map = model(torch.from_numpy(X[i]).float().unsqueeze(dim=0))\n",
    "            seg_preds = seg_logits.argmax(dim=1)\n",
    "            log_pred   = reg_map.squeeze(1)         \n",
    "            rate_pred  = torch.exp(log_pred) - eps   \n",
    "            rate_pred  = torch.clamp(rate_pred, min=0.0)\n",
    "\n",
    "    \n",
    "        seg_numpy  = seg_preds.cpu().numpy().squeeze()   \n",
    "        rate_numpy = rate_pred.cpu().numpy().squeeze()   \n",
    "        Rain_rate[i] = rate_numpy\n",
    "\n",
    "        Flag[i] = seg_numpy\n",
    "        \n",
    "           \n",
    "    return np.concatenate((np.expand_dims(Flag, axis=1),np.expand_dims(Rain_rate, axis=1)), axis=1)  \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def extract_patches_with_padding(data, patch_size=16):\n",
    "    C, H, W = data.shape\n",
    "    pad_h = (patch_size - (H % patch_size)) % patch_size\n",
    "    padded = np.pad(data, ((0, 0), (0, pad_h), (0, 0)), mode='constant')\n",
    "\n",
    "    H_padded = padded.shape[1]\n",
    "    patches = []\n",
    "    for i in range(0, H_padded, patch_size):\n",
    "        for j in range(0, W, patch_size):\n",
    "            patch = padded[:, i:i+patch_size, j:j+patch_size]\n",
    "            patches.append(patch)\n",
    "    \n",
    "    return np.stack(patches) \n",
    "\n",
    "def reconstruct_from_patches(patches, original_height=2236, width=176, patch_size=16):\n",
    "    C = patches.shape[1]\n",
    "    h_patches = (original_height + patch_size - 1) // patch_size\n",
    "    w_patches = width // patch_size\n",
    "    \n",
    "    full_height = h_patches * patch_size\n",
    "    reconstructed = np.zeros((C, full_height, width), dtype=patches.dtype)\n",
    "\n",
    "    idx = 0\n",
    "    for i in range(0, full_height, patch_size):\n",
    "        for j in range(0, width, patch_size):\n",
    "            reconstructed[:, i:i+patch_size, j:j+patch_size] = patches[idx]\n",
    "            idx += 1\n",
    "\n",
    "    return reconstructed[:, :original_height, :]\n",
    "\n",
    "\n",
    "\n",
    "Filename = '1C.NPP.ATMS.XCAL2019-V.20230712-S071701-E085830.060650.V07A.ERA5.HDF5'\n",
    "\n",
    "\n",
    "model = UNetWithTransformerSkips(in_channels=38, n_classes=2)\n",
    "model.load_state_dict(torch.load(\"model_weights_R22.pth\", map_location=torch.device('cpu')))\n",
    "std = np.load('std.npy')\n",
    "mean = np.load('mean.npy')\n",
    "\n",
    "Flag, Rate = reconstruct_from_patches(RerievalDL(model,extract_patches_with_padding(Expand(CollectingData(Filename))[:38]),mean,std))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
